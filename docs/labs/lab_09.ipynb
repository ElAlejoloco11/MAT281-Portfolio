{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q413iXOUoZnM"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fralfaro/MAT281_2024/blob/main/docs/labs/lab_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "# MAT281 - Laboratorio N°09"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yz9ARBloZnR"
      },
      "source": [
        "<a id='p1'></a>\n",
        "## I.- Problema 01\n",
        "\n",
        "\n",
        "<img src=\"https://www.svgrepo.com/show/1064/virus.svg\" width = \"300\" align=\"center\"/>\n",
        "\n",
        "El **cáncer de mama** es una proliferación maligna de células epiteliales en los conductos o lobulillos mamarios. Esta enfermedad clonal surge cuando una célula adquiere, a través de mutaciones, la capacidad de dividirse sin control, formando un tumor que puede invadir tejidos vecinos y propagarse a otras partes del cuerpo.\n",
        "\n",
        "El conjunto de datos `BC.csv` contiene información sobre pacientes con tumores (benignos o malignos) y diversas características del tumor, calculadas a partir de imágenes digitalizadas de aspirados con aguja fina (FNA) de masas mamarias. Estas características describen los núcleos celulares en la imagen y permiten diferenciar entre tumores benignos y malignos.\n",
        "\n",
        "A continuación, cargamos el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "lEtHyuJRoZnT",
        "outputId": "0770256a-569b-4541-b04b-fdf6c247be2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hs/jrlgxzq10918y2jbc8cddg500000gn/T/ipykernel_7533/37940209.py:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['diagnosis'] = df['diagnosis'].replace({'M': 1, 'B': 0}).astype(int)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>842302</th>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842517</th>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84300903</th>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84348301</th>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84358402</th>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "id                                                                          \n",
              "842302            1        17.99         10.38          122.80     1001.0   \n",
              "842517            1        20.57         17.77          132.90     1326.0   \n",
              "84300903          1        19.69         21.25          130.00     1203.0   \n",
              "84348301          1        11.42         20.38           77.58      386.1   \n",
              "84358402          1        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "          smoothness_mean  compactness_mean  concavity_mean  \\\n",
              "id                                                            \n",
              "842302            0.11840           0.27760          0.3001   \n",
              "842517            0.08474           0.07864          0.0869   \n",
              "84300903          0.10960           0.15990          0.1974   \n",
              "84348301          0.14250           0.28390          0.2414   \n",
              "84358402          0.10030           0.13280          0.1980   \n",
              "\n",
              "          concave points_mean  symmetry_mean  ...  radius_worst  \\\n",
              "id                                            ...                 \n",
              "842302                0.14710         0.2419  ...         25.38   \n",
              "842517                0.07017         0.1812  ...         24.99   \n",
              "84300903              0.12790         0.2069  ...         23.57   \n",
              "84348301              0.10520         0.2597  ...         14.91   \n",
              "84358402              0.10430         0.1809  ...         22.54   \n",
              "\n",
              "          texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "id                                                                       \n",
              "842302            17.33           184.60      2019.0            0.1622   \n",
              "842517            23.41           158.80      1956.0            0.1238   \n",
              "84300903          25.53           152.50      1709.0            0.1444   \n",
              "84348301          26.50            98.87       567.7            0.2098   \n",
              "84358402          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "          compactness_worst  concavity_worst  concave points_worst  \\\n",
              "id                                                                   \n",
              "842302               0.6656           0.7119                0.2654   \n",
              "842517               0.1866           0.2416                0.1860   \n",
              "84300903             0.4245           0.4504                0.2430   \n",
              "84348301             0.8663           0.6869                0.2575   \n",
              "84358402             0.2050           0.4000                0.1625   \n",
              "\n",
              "          symmetry_worst  fractal_dimension_worst  \n",
              "id                                                 \n",
              "842302            0.4601                  0.11890  \n",
              "842517            0.2750                  0.08902  \n",
              "84300903          0.3613                  0.08758  \n",
              "84348301          0.6638                  0.17300  \n",
              "84358402          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importar librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importar herramientas de Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Configuración de gráficos\n",
        "%matplotlib inline\n",
        "sns.set_palette(\"deep\", desat=0.6)\n",
        "sns.set(rc={'figure.figsize': (11.7, 8.27)})\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/BC.csv\")\n",
        "df.set_index('id', inplace=True)\n",
        "\n",
        "# Transformación de la variable objetivo\n",
        "df['diagnosis'] = df['diagnosis'].replace({'M': 1, 'B': 0}).astype(int)\n",
        "\n",
        "# Visualizar las primeras filas del DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhqGIF62oZnW"
      },
      "source": [
        "Con base en la información presentada, responde las siguientes preguntas. Asegúrate de incluir el código necesario para realizar los análisis requeridos y proporciona una breve explicación que describa tus resultados y el proceso que seguiste.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_25-ZyroZnX"
      },
      "source": [
        "\n",
        "\n",
        "1. Realice un análisis exploratorio del conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I65DdnyGoZnX",
        "outputId": "3d2f726c-0a40-42d8-c73c-2f9416587be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 569 entries, 842302 to 92751\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   diagnosis                569 non-null    int64  \n",
            " 1   radius_mean              569 non-null    float64\n",
            " 2   texture_mean             569 non-null    float64\n",
            " 3   perimeter_mean           569 non-null    float64\n",
            " 4   area_mean                569 non-null    float64\n",
            " 5   smoothness_mean          569 non-null    float64\n",
            " 6   compactness_mean         569 non-null    float64\n",
            " 7   concavity_mean           569 non-null    float64\n",
            " 8   concave points_mean      569 non-null    float64\n",
            " 9   symmetry_mean            569 non-null    float64\n",
            " 10  fractal_dimension_mean   569 non-null    float64\n",
            " 11  radius_se                569 non-null    float64\n",
            " 12  texture_se               569 non-null    float64\n",
            " 13  perimeter_se             569 non-null    float64\n",
            " 14  area_se                  569 non-null    float64\n",
            " 15  smoothness_se            569 non-null    float64\n",
            " 16  compactness_se           569 non-null    float64\n",
            " 17  concavity_se             569 non-null    float64\n",
            " 18  concave points_se        569 non-null    float64\n",
            " 19  symmetry_se              569 non-null    float64\n",
            " 20  fractal_dimension_se     569 non-null    float64\n",
            " 21  radius_worst             569 non-null    float64\n",
            " 22  texture_worst            569 non-null    float64\n",
            " 23  perimeter_worst          569 non-null    float64\n",
            " 24  area_worst               569 non-null    float64\n",
            " 25  smoothness_worst         569 non-null    float64\n",
            " 26  compactness_worst        569 non-null    float64\n",
            " 27  concavity_worst          569 non-null    float64\n",
            " 28  concave points_worst     569 non-null    float64\n",
            " 29  symmetry_worst           569 non-null    float64\n",
            " 30  fractal_dimension_worst  569 non-null    float64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 142.2 KB\n",
            "diagnosis                  0\n",
            "radius_mean                0\n",
            "texture_mean               0\n",
            "perimeter_mean             0\n",
            "area_mean                  0\n",
            "smoothness_mean            0\n",
            "compactness_mean           0\n",
            "concavity_mean             0\n",
            "concave points_mean        0\n",
            "symmetry_mean              0\n",
            "fractal_dimension_mean     0\n",
            "radius_se                  0\n",
            "texture_se                 0\n",
            "perimeter_se               0\n",
            "area_se                    0\n",
            "smoothness_se              0\n",
            "compactness_se             0\n",
            "concavity_se               0\n",
            "concave points_se          0\n",
            "symmetry_se                0\n",
            "fractal_dimension_se       0\n",
            "radius_worst               0\n",
            "texture_worst              0\n",
            "perimeter_worst            0\n",
            "area_worst                 0\n",
            "smoothness_worst           0\n",
            "compactness_worst          0\n",
            "concavity_worst            0\n",
            "concave points_worst       0\n",
            "symmetry_worst             0\n",
            "fractal_dimension_worst    0\n",
            "dtype: int64\n",
            "Hay 0 filas duplicadas\n",
            "diagnosis\n",
            "0    357\n",
            "1    212\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# FIXME\n",
        "df.info()\n",
        "df.describe()\n",
        "print(df.isnull().sum())\n",
        "print(f'Hay {df.duplicated().sum()} filas duplicadas')\n",
        "print(df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqaGCwQzoZnY"
      },
      "source": [
        "2. Normalizar las variables numéricas con el método **StandardScaler**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fXH_cMb_oZnZ"
      },
      "outputs": [],
      "source": [
        "# FIXME\n",
        "#Separar en vector target y en caracteristicas\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "#Como haremos optimización de hiperparámetros, separaremos en conjunto de test, train y validación.\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "#Normalizamos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI-Kz8kioZna"
      },
      "source": [
        "3. Realizar un método de reducción de dimensionalidad visto en clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3vBJrv1oZna",
        "outputId": "545a0d90-7cad-4c4c-db02-027bed6a16c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: np.float64(0.44578650065968295), 2: np.float64(0.6429583348759293), 3: np.float64(0.7376739377574357), 4: np.float64(0.799909962089413), 5: np.float64(0.8504783162396501), 6: np.float64(0.8926644635873201), 7: np.float64(0.9140121480354398), 8: np.float64(0.9297820418446238), 9: np.float64(0.9423040436785775), 10: np.float64(0.953316661677575), 11: np.float64(0.9631769582402304), 12: np.float64(0.972553425490332), 13: np.float64(0.9793765264482869), 14: np.float64(0.9847531282317254), 15: np.float64(0.9873498557276728), 16: np.float64(0.989773196135695), 17: np.float64(0.9918177760012158), 18: np.float64(0.9936401386037104), 19: np.float64(0.9950014590275786), 20: np.float64(0.9960447289836011), 21: np.float64(0.9968760140071131), 22: np.float64(0.9976525876123885), 23: np.float64(0.9983331392303412), 24: np.float64(0.9989680868717268), 25: np.float64(0.9994472900233223), 26: np.float64(0.9997210008930144), 27: np.float64(0.9999289668269118), 28: np.float64(0.9999733304082424), 29: np.float64(0.999995876289505), 30: np.float64(1.0)}\n"
          ]
        }
      ],
      "source": [
        "# FIXME\n",
        "#PCA\n",
        "#Veamos la varianza acumulada para saber cuantas componentes se necesitan. Conformemonos con tener 90%\n",
        "pca_scores = {}\n",
        "for n_components in range(1, len(X_train_scaled[0]) + 1):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(X_train_scaled)\n",
        "    pca_scores[n_components] = pca.explained_variance_ratio_.sum()\n",
        "print(pca_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ctAX6RGF1Z5A"
      },
      "outputs": [],
      "source": [
        "#Con 7 caracteristicas basta\n",
        "pca = PCA(n_components=7)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_val_pca = pca.transform(X_val_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8GY26iWoZna"
      },
      "source": [
        "4. Aplique al menos tres modelos de clasificación distintos. Para cada uno de los modelos escogidos, realice una optimización de los hiperparámetros. además, calcule las respectivas métricas. Concluya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvTRga2ooZnb",
        "outputId": "4d3a337c-4e00-4b3e-c996-2a855a3b0d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Métricas iniciales Logistic Regression:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 95.6140350877193%\n",
            "Precision: 93.33333333333333%\n",
            "Recall: 95.45454545454545%\n",
            "F1: 94.3820224719101% \n",
            "\n",
            "Mejores hiperparámetros Logistic Regression:\n",
            "{'C': 0.1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            " Métricas optimizadas Logistic Regression:\n",
            "Accuracy: 95.6140350877193%\n",
            "Precision: 95.34883720930233%\n",
            "Recall: 93.18181818181817%\n",
            "F1: 94.25287356321839%\n",
            "\n",
            "Métricas finales Logistic Regression:\n",
            "Accuracy: 97.36842105263158%\n",
            "Precision: 95.45454545454545%\n",
            "Recall: 97.67441860465115%\n",
            "F1: 96.55172413793103% \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Métricas iniciales Support Vector Machine:\n",
            "Accuracy: 94.73684210526315%\n",
            "Precision: 93.18181818181817%\n",
            "Recall: 93.18181818181817%\n",
            "F1: 93.18181818181817% \n",
            "\n",
            "Mejores hiperparámetros SVM:\n",
            "{'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "\n",
            " Métricas optimizadas SVM:\n",
            "Accuracy: 95.6140350877193%\n",
            "Precision: 93.33333333333333%\n",
            "Recall: 95.45454545454545%\n",
            "F1: 94.3820224719101%\n",
            "\n",
            "Métricas finales SVM:\n",
            "Accuracy: 96.49122807017544%\n",
            "Precision: 95.34883720930233%\n",
            "Recall: 95.34883720930233%\n",
            "F1: 95.34883720930233%\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Métricas iniciales Random Forest:\n",
            "Accuracy: 93.85964912280701%\n",
            "Precision: 93.02325581395348%\n",
            "Recall: 90.9090909090909%\n",
            "F1: 91.95402298850574% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros Random Forest:\n",
            "{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            " Métricas optimizadas Random Forest:\n",
            "Accuracy: 94.73684210526315%\n",
            "Precision: 93.18181818181817%\n",
            "Recall: 93.18181818181817%\n",
            "F1: 93.18181818181817%\n",
            "\n",
            "Métricas finales Random Forest:\n",
            "Accuracy: 96.49122807017544%\n",
            "Precision: 95.34883720930233%\n",
            "Recall: 95.34883720930233%\n",
            "F1: 95.34883720930233%\n"
          ]
        }
      ],
      "source": [
        "# FIXME\n",
        "#Logistic Regression\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train_pca, y_train)\n",
        "y_pred = LR.predict(X_val_pca)\n",
        "print('Métricas iniciales Logistic Regression:')\n",
        "print(f'Accuracy: {accuracy_score(y_val, y_pred)*100}%')\n",
        "print(f'Precision: {precision_score(y_val, y_pred)*100}%')\n",
        "print(f'Recall: {recall_score(y_val, y_pred)*100}%')\n",
        "print(f'F1: {f1_score(y_val, y_pred)*100}% \\n')\n",
        "#Optimizacion de Hiperparámetros con GridSearch\n",
        "LR_params ={\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs', 'liblinear'],\n",
        "    'max_iter': [200, 500, 1000]\n",
        "}\n",
        "LR_grid = GridSearchCV(estimator=  LogisticRegression(), param_grid=LR_params, cv=5)\n",
        "LR_grid.fit(X_train_pca, y_train)\n",
        "print('Mejores hiperparámetros Logistic Regression:')\n",
        "print(LR_grid.best_params_)\n",
        "LR_best = LogisticRegression(**LR_grid.best_params_)\n",
        "LR_best.fit(X_train_pca, y_train)\n",
        "y_pred = LR_best.predict(X_val_pca)\n",
        "print('\\n Métricas optimizadas Logistic Regression:')\n",
        "print(f'Accuracy: {accuracy_score(y_val, y_pred)*100}%')\n",
        "print(f'Precision: {precision_score(y_val, y_pred)*100}%')\n",
        "print(f'Recall: {recall_score(y_val, y_pred)*100}%')\n",
        "print(f'F1: {f1_score(y_val, y_pred)*100}%\\n')\n",
        "#Para este modelo no se aprecia mejoría entre los parametros iniciales y los optimizados.\n",
        "y_pred_test = LR_best.predict(X_test_pca)\n",
        "print('Métricas finales Logistic Regression:')\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred_test)*100}%')\n",
        "print(f'Precision: {precision_score(y_test, y_pred_test)*100}%')\n",
        "print(f'Recall: {recall_score(y_test, y_pred_test)*100}%')\n",
        "print(f'F1: {f1_score(y_test, y_pred_test)*100}% \\n \\n \\n \\n')\n",
        "\n",
        "#Support Vector Machine\n",
        "svm = SVC()\n",
        "svm.fit(X_train_pca, y_train)\n",
        "y_pred = svm.predict(X_val_pca)\n",
        "print('Métricas iniciales Support Vector Machine:')\n",
        "print(f'Accuracy: {accuracy_score(y_val, y_pred)*100}%')\n",
        "print(f'Precision: {precision_score(y_val, y_pred)*100}%')\n",
        "print(f'Recall: {recall_score(y_val, y_pred)*100}%')\n",
        "print(f'F1: {f1_score(y_val, y_pred)*100}% \\n')\n",
        "#Optimizacion de Hiperparámetros con GridSearch\n",
        "SVM_params = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'degree': [2, 3, 4]\n",
        "}\n",
        "svm_grid = GridSearchCV(estimator=SVC(), param_grid=SVM_params, cv=5)\n",
        "svm_grid.fit(X_train_pca, y_train)\n",
        "print('Mejores hiperparámetros SVM:')\n",
        "print(svm_grid.best_params_)\n",
        "svm_best = SVC(**svm_grid.best_params_)\n",
        "svm_best.fit(X_train_pca, y_train)\n",
        "y_pred = svm_best.predict(X_val_pca)\n",
        "print('\\n Métricas optimizadas SVM:')\n",
        "print(f'Accuracy: {accuracy_score(y_val, y_pred)*100}%')\n",
        "print(f'Precision: {precision_score(y_val, y_pred)*100}%')\n",
        "print(f'Recall: {recall_score(y_val, y_pred)*100}%')\n",
        "print(f'F1: {f1_score(y_val, y_pred)*100}%\\n')\n",
        "y_pred_test = svm.predict(X_test_pca)\n",
        "print('Métricas finales SVM:')\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred_test)*100}%')\n",
        "print(f'Precision: {precision_score(y_test, y_pred_test)*100}%')\n",
        "print(f'Recall: {recall_score(y_test, y_pred_test)*100}%')\n",
        "print(f'F1: {f1_score(y_test, y_pred_test)*100}%\\n \\n \\n \\n')\n",
        "#Random Forest Classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_pca, y_train)\n",
        "y_pred = rf.predict(X_val_pca)\n",
        "print('Métricas iniciales Random Forest:')\n",
        "print(f'Accuracy: {accuracy_score(y_val, y_pred)*100}%')\n",
        "print(f'Precision: {precision_score(y_val, y_pred)*100}%')\n",
        "print(f'Recall: {recall_score(y_val, y_pred)*100}%')\n",
        "print(f'F1: {f1_score(y_val, y_pred)*100}% \\n')\n",
        "#Optimizacion de. Hiperoparámetros con GridSearch\n",
        "RF_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "}\n",
        "rf_grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=RF_params, cv=5)\n",
        "rf_grid.fit(X_train_pca, y_train)\n",
        "print('Mejores hiperparámetros Random Forest:')\n",
        "print(rf_grid.best_params_)\n",
        "rf_best = RandomForestClassifier(**rf_grid.best_params_)\n",
        "rf_best.fit(X_train_pca, y_train)\n",
        "y_pred = rf_best.predict(X_val_pca)\n",
        "print('\\n Métricas optimizadas Random Forest:')\n",
        "print(f'Accuracy: {accuracy_score(y_val, y_pred)*100}%')\n",
        "print(f'Precision: {precision_score(y_val, y_pred)*100}%')\n",
        "print(f'Recall: {recall_score(y_val, y_pred)*100}%')\n",
        "print(f'F1: {f1_score(y_val, y_pred)*100}%\\n')\n",
        "y_pred_test = rf_best.predict(X_test_pca)\n",
        "print('Métricas finales Random Forest:')\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred_test)*100}%')\n",
        "print(f'Precision: {precision_score(y_test, y_pred_test)*100}%')\n",
        "print(f'Recall: {recall_score(y_test, y_pred_test)*100}%')\n",
        "print(f'F1: {f1_score(y_test, y_pred_test)*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apqUFlWCx-rF"
      },
      "source": [
        "En el modelo Logistic Regression no se vieron cambios en las métricas. Sin embargo, tanto Support Vector Machine como en Random Forest se vió una mejoría en el desempeño al optimizar los hiperparámetros de ambos modelos. Sn embargo, la mejoría fue de  alrededor de 1% para  ambos modelos, y  el tiempo que tardó en optimizar los parámetros dependía fuertemente de la cantidad de hiperparámetros a optimizar, por lo que la herramienta de optimizar hiperparámetros debe ser utilizada con precaución, intentando tener la mayor cantidad de información sobre los datos para saber que hiperpárametros sirve optimizar y cuales no. Por ejemplo, para logistic regresion , dependiendo de como se distribuyen los datos, y que tan reelevantes son las variables que se trabajan uno debe escoger una función de pérdida adecuada."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
